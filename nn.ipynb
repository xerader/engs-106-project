{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "bank_marketing = fetch_ucirepo(id=222) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = bank_marketing.data.features \n",
    "y = bank_marketing.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(bank_marketing.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(bank_marketing.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Assume X and y are your original DataFrames/Series.\n",
    "# Factorize each categorical column in X to ensure numeric values.\n",
    "X_factorized = X.copy()\n",
    "for col in X.columns:\n",
    "    X_factorized[col], _ = pd.factorize(X[col])\n",
    "\n",
    "# Factorize and one-hot encode the target variable.\n",
    "y_numeric, uniques = pd.factorize(y.values.ravel())\n",
    "y_tensor = torch.tensor(y_numeric, dtype=torch.long)\n",
    "y_onehot = F.one_hot(y_tensor, num_classes=2).numpy()\n",
    "\n",
    "# Create train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_factorized, y_onehot, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# --- Neural Network Class Definition ---\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, lr=0.001):\n",
    "        self.lr = lr\n",
    "        # Initialize weights with small random values and biases with zeros.\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1) * lr\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2) * lr\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "        \n",
    "        self.W3 = np.random.randn(hidden_size2, output_size) * lr\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Implement a 3 layer neural network forward propagation\"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        self.X = X\n",
    "        self.z1 = np.dot(self.X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        output = self.softmax(self.z3)\n",
    "        return output \n",
    "    \n",
    "    def compute_loss(self, y, output, weight=None):\n",
    "        \"\"\"\n",
    "        Compute Cross Entropy Loss, where y is one-hot encoded and output is the softmax output.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        if weight is not None:\n",
    "            weight = weight.reshape(1, -1)\n",
    "            loss = -1/m * np.sum(weight * (y * np.log(output)))\n",
    "        else:\n",
    "            loss = -1/m * np.sum(y * np.log(output))\n",
    "        return loss\n",
    "            \n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = y.shape[0]\n",
    "        # Output layer gradients\n",
    "        dZ3 = output - y\n",
    "        dW3 = np.dot(self.a2.T, dZ3) / m\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Propagate to second hidden layer\n",
    "        dA2 = np.dot(dZ3, self.W3.T)\n",
    "        dZ2 = dA2 * (self.a2 * (1 - self.a2))\n",
    "        dW2 = np.dot(self.a1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Propagate to first hidden layer\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * (self.a1 * (1 - self.a1))\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.W3 -= self.lr * dW3\n",
    "        self.b3 -= self.lr * db3\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size=64):\n",
    "        \"\"\"Batch Gradient Descent training\"\"\"\n",
    "        m = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.choice(m, batch_size, replace=False)\n",
    "            # Use .iloc for DataFrame and direct indexing for numpy arrays.\n",
    "            X_batch = X.iloc[indices]\n",
    "            y_batch = y[indices]\n",
    "        \n",
    "            output = self.forward(X_batch)\n",
    "            loss = self.compute_loss(y_batch, output)\n",
    "            self.backward(X_batch, y_batch, output)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Loss at epoch {epoch}: {loss}')\n",
    "                \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# --- Cross-Validation Training ---\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "input_size = 16\n",
    "output_size = 2\n",
    "hidden_size1 = 32\n",
    "hidden_size2 = 16\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "fold = 1\n",
    "\n",
    "# Lists to collect metrics for each fold\n",
    "acc_scores = []\n",
    "prec_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "# Reset indices for X_train to ensure proper integer indexing\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "# y_train is already a NumPy array\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Use .iloc for DataFrame indexing and numpy indexing for y_train\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    nn = NN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    print(f\"Training on fold {fold}: {X_tr.shape[0]} samples\")\n",
    "    \n",
    "    nn.train(X_tr, y_tr, epochs=5000, batch_size=2000)\n",
    "    \n",
    "    # Evaluate on the validation set from the current fold\n",
    "    y_val_trye = np.argmax(y_val, axis=1)\n",
    "    # obtain the predicted class probabilities\n",
    "    y_val_pred = nn.predict(X_val)\n",
    "    y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "    \n",
    "    # Compute and print accuracy and classification report for the current fold\n",
    "    binary_acc = accuracy_score(y_val_trye, y_val_pred)\n",
    "    acc_scores.append(binary_acc)\n",
    "    print(f\"Accuracy on fold {fold}: {binary_acc}\")\n",
    "    print(classification_report(y_val_trye, y_val_pred))\n",
    "    \n",
    "    fold += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "y_test_pred = nn.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "print(classification_report(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
