{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.functional import one_hot\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 222, 'name': 'Bank Marketing', 'repository_url': 'https://archive.ics.uci.edu/dataset/222/bank+marketing', 'data_url': 'https://archive.ics.uci.edu/static/public/222/data.csv', 'abstract': 'The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 45211, 'num_features': 16, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Occupation', 'Marital Status', 'Education Level'], 'target_col': ['y'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2014, 'last_updated': 'Fri Aug 18 2023', 'dataset_doi': '10.24432/C5K306', 'creators': ['S. Moro', 'P. Rita', 'P. Cortez'], 'intro_paper': {'ID': 277, 'type': 'NATIVE', 'title': 'A data-driven approach to predict the success of bank telemarketing', 'authors': 'SÃ©rgio Moro, P. Cortez, P. Rita', 'venue': 'Decision Support Systems', 'year': 2014, 'journal': None, 'DOI': '10.1016/j.dss.2014.03.001', 'URL': 'https://www.semanticscholar.org/paper/cab86052882d126d43f72108c6cb41b295cc8a9e', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': \"The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \\n\\nThere are four datasets: \\n1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\\n2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\\n3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). \\n4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). \\nThe smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM). \\n\\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Input variables:\\n   # bank client data:\\n   1 - age (numeric)\\n   2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\\n                                       \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \\n   3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\\n   4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\\n   5 - default: has credit in default? (binary: \"yes\",\"no\")\\n   6 - balance: average yearly balance, in euros (numeric) \\n   7 - housing: has housing loan? (binary: \"yes\",\"no\")\\n   8 - loan: has personal loan? (binary: \"yes\",\"no\")\\n   # related with the last contact of the current campaign:\\n   9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \\n  10 - day: last contact day of the month (numeric)\\n  11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\\n  12 - duration: last contact duration, in seconds (numeric)\\n   # other attributes:\\n  13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\\n  14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\\n  15 - previous: number of contacts performed before this campaign and for this client (numeric)\\n  16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\\n\\n  Output variable (desired target):\\n  17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\\n', 'citation': None}}\n",
      "           name     role         type      demographic  \\\n",
      "0           age  Feature      Integer              Age   \n",
      "1           job  Feature  Categorical       Occupation   \n",
      "2       marital  Feature  Categorical   Marital Status   \n",
      "3     education  Feature  Categorical  Education Level   \n",
      "4       default  Feature       Binary             None   \n",
      "5       balance  Feature      Integer             None   \n",
      "6       housing  Feature       Binary             None   \n",
      "7          loan  Feature       Binary             None   \n",
      "8       contact  Feature  Categorical             None   \n",
      "9   day_of_week  Feature         Date             None   \n",
      "10        month  Feature         Date             None   \n",
      "11     duration  Feature      Integer             None   \n",
      "12     campaign  Feature      Integer             None   \n",
      "13        pdays  Feature      Integer             None   \n",
      "14     previous  Feature      Integer             None   \n",
      "15     poutcome  Feature  Categorical             None   \n",
      "16            y   Target       Binary             None   \n",
      "\n",
      "                                          description  units missing_values  \n",
      "0                                                None   None             no  \n",
      "1   type of job (categorical: 'admin.','blue-colla...   None             no  \n",
      "2   marital status (categorical: 'divorced','marri...   None             no  \n",
      "3   (categorical: 'basic.4y','basic.6y','basic.9y'...   None             no  \n",
      "4                              has credit in default?   None             no  \n",
      "5                              average yearly balance  euros             no  \n",
      "6                                   has housing loan?   None             no  \n",
      "7                                  has personal loan?   None             no  \n",
      "8   contact communication type (categorical: 'cell...   None            yes  \n",
      "9                        last contact day of the week   None             no  \n",
      "10  last contact month of year (categorical: 'jan'...   None             no  \n",
      "11   last contact duration, in seconds (numeric). ...   None             no  \n",
      "12  number of contacts performed during this campa...   None             no  \n",
      "13  number of days that passed by after the client...   None            yes  \n",
      "14  number of contacts performed before this campa...   None             no  \n",
      "15  outcome of the previous marketing campaign (ca...   None            yes  \n",
      "16          has the client subscribed a term deposit?   None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "bank_marketing = fetch_ucirepo(id=222) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = bank_marketing.data.features \n",
    "y = bank_marketing.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(bank_marketing.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(bank_marketing.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1: 28934 samples\n",
      "Loss at epoch 0: 0.6909256103608725\n",
      "Loss at epoch 100: 0.5789004065184495\n",
      "Loss at epoch 200: 0.5022362136145423\n",
      "Loss at epoch 300: 0.45694508279455787\n",
      "Loss at epoch 400: 0.4301940286524067\n",
      "Loss at epoch 500: 0.4205409704616166\n",
      "Loss at epoch 600: 0.40977273973983486\n",
      "Loss at epoch 700: 0.38209349574710516\n",
      "Loss at epoch 800: 0.39877908493148584\n",
      "Loss at epoch 900: 0.37343478629610866\n",
      "Loss at epoch 1000: 0.3681330627106137\n",
      "Loss at epoch 1100: 0.3833486964029281\n",
      "Loss at epoch 1200: 0.35639345521702176\n",
      "Loss at epoch 1300: 0.35723940594709\n",
      "Loss at epoch 1400: 0.35769105341565616\n",
      "Loss at epoch 1500: 0.3594367685638496\n",
      "Loss at epoch 1600: 0.3467904077284841\n",
      "Loss at epoch 1700: 0.33776097026948354\n",
      "Loss at epoch 1800: 0.3659823199280362\n",
      "Loss at epoch 1900: 0.3440632794931606\n",
      "Loss at epoch 2000: 0.354117730140921\n",
      "Loss at epoch 2100: 0.3635752910519849\n",
      "Loss at epoch 2200: 0.3546973557458748\n",
      "Loss at epoch 2300: 0.35350817298897835\n",
      "Loss at epoch 2400: 0.3514614379017836\n",
      "Loss at epoch 2500: 0.3581656423883712\n",
      "Loss at epoch 2600: 0.33058834533849984\n",
      "Loss at epoch 2700: 0.3344555458222072\n",
      "Loss at epoch 2800: 0.36392844607672703\n",
      "Loss at epoch 2900: 0.3728190356828683\n",
      "Loss at epoch 3000: 0.3688473530147819\n",
      "Loss at epoch 3100: 0.3399997126803852\n",
      "Loss at epoch 3200: 0.36590621216966757\n",
      "Loss at epoch 3300: 0.3658878762761513\n",
      "Loss at epoch 3400: 0.3297327894811049\n",
      "Loss at epoch 3500: 0.3506659937299609\n",
      "Loss at epoch 3600: 0.3577404571626431\n",
      "Loss at epoch 3700: 0.3675133322096131\n",
      "Loss at epoch 3800: 0.3917270465002895\n",
      "Loss at epoch 3900: 0.347498823674189\n",
      "Loss at epoch 4000: 0.35662817280947323\n",
      "Loss at epoch 4100: 0.37470607727715\n",
      "Loss at epoch 4200: 0.3646707800444968\n",
      "Loss at epoch 4300: 0.35951859707414646\n",
      "Loss at epoch 4400: 0.36365932976024645\n",
      "Loss at epoch 4500: 0.37775345974294916\n",
      "Loss at epoch 4600: 0.3606850058389681\n",
      "Loss at epoch 4700: 0.34933739599035746\n",
      "Loss at epoch 4800: 0.33124326295953754\n",
      "Loss at epoch 4900: 0.37161556129000495\n",
      "Fold 1 - Accuracy: 0.8801, Precision: 0.0000, Recall: 0.0000\n",
      "Training on fold 2: 28934 samples\n",
      "Loss at epoch 0: 0.6931792463246227\n",
      "Loss at epoch 100: 0.5769635552106793\n",
      "Loss at epoch 200: 0.5041243093551399\n",
      "Loss at epoch 300: 0.4574745806964064\n",
      "Loss at epoch 400: 0.43584131338638754\n",
      "Loss at epoch 500: 0.41614435694749957\n",
      "Loss at epoch 600: 0.39286465032833884\n",
      "Loss at epoch 700: 0.3977102479376432\n",
      "Loss at epoch 800: 0.4037617316419861\n",
      "Loss at epoch 900: 0.3665708184480879\n",
      "Loss at epoch 1000: 0.37279994918185855\n",
      "Loss at epoch 1100: 0.35701096379000347\n",
      "Loss at epoch 1200: 0.37248277068854463\n",
      "Loss at epoch 1300: 0.37714559368330935\n",
      "Loss at epoch 1400: 0.3471667605160688\n",
      "Loss at epoch 1500: 0.3640496439629156\n",
      "Loss at epoch 1600: 0.35064386343401904\n",
      "Loss at epoch 1700: 0.36369155350845744\n",
      "Loss at epoch 1800: 0.35492233939106677\n",
      "Loss at epoch 1900: 0.3489509358437838\n",
      "Loss at epoch 2000: 0.3675712228900695\n",
      "Loss at epoch 2100: 0.3607428208849467\n",
      "Loss at epoch 2200: 0.37693306881434596\n",
      "Loss at epoch 2300: 0.3642193422507944\n",
      "Loss at epoch 2400: 0.32623787358789674\n",
      "Loss at epoch 2500: 0.34360482992670854\n",
      "Loss at epoch 2600: 0.35906927674446126\n",
      "Loss at epoch 2700: 0.35983626541455527\n",
      "Loss at epoch 2800: 0.3482379813075391\n",
      "Loss at epoch 2900: 0.3263446712657049\n",
      "Loss at epoch 3000: 0.3677093617674768\n",
      "Loss at epoch 3100: 0.35207018655448613\n",
      "Loss at epoch 3200: 0.3778476581189553\n",
      "Loss at epoch 3300: 0.3488529977600792\n",
      "Loss at epoch 3400: 0.3657281520113952\n",
      "Loss at epoch 3500: 0.3448835928953279\n",
      "Loss at epoch 3600: 0.3528199456346382\n",
      "Loss at epoch 3700: 0.35874979212444486\n",
      "Loss at epoch 3800: 0.3506876758504601\n",
      "Loss at epoch 3900: 0.3705488414970521\n",
      "Loss at epoch 4000: 0.34643383923103044\n",
      "Loss at epoch 4100: 0.3706500400818439\n",
      "Loss at epoch 4200: 0.3566585310109656\n",
      "Loss at epoch 4300: 0.3745200497733541\n",
      "Loss at epoch 4400: 0.3527346710808886\n",
      "Loss at epoch 4500: 0.3514974819555449\n",
      "Loss at epoch 4600: 0.37442471338548877\n",
      "Loss at epoch 4700: 0.3645648388097143\n",
      "Loss at epoch 4800: 0.3786382319197044\n",
      "Loss at epoch 4900: 0.3386387438053506\n",
      "Fold 2 - Accuracy: 0.8847, Precision: 0.0000, Recall: 0.0000\n",
      "Training on fold 3: 28934 samples\n",
      "Loss at epoch 0: 0.6930706816720126\n",
      "Loss at epoch 100: 0.5747118102182817\n",
      "Loss at epoch 200: 0.503353776883855\n",
      "Loss at epoch 300: 0.46890381863198194\n",
      "Loss at epoch 400: 0.43169665934649665\n",
      "Loss at epoch 500: 0.42845162000721554\n",
      "Loss at epoch 600: 0.4039134326329878\n",
      "Loss at epoch 700: 0.3744415340793307\n",
      "Loss at epoch 800: 0.3696565203551713\n",
      "Loss at epoch 900: 0.3625091326580287\n",
      "Loss at epoch 1000: 0.356796058675693\n",
      "Loss at epoch 1100: 0.34871168638080235\n",
      "Loss at epoch 1200: 0.37311403566295176\n",
      "Loss at epoch 1300: 0.37001943025439\n",
      "Loss at epoch 1400: 0.35311448096579373\n",
      "Loss at epoch 1500: 0.3458862168951865\n",
      "Loss at epoch 1600: 0.346716041707506\n",
      "Loss at epoch 1700: 0.3709844292192123\n",
      "Loss at epoch 1800: 0.34914888588521653\n",
      "Loss at epoch 1900: 0.382903687684797\n",
      "Loss at epoch 2000: 0.37236062209960424\n",
      "Loss at epoch 2100: 0.3682700174879908\n",
      "Loss at epoch 2200: 0.34316117934678836\n",
      "Loss at epoch 2300: 0.3525155269986724\n",
      "Loss at epoch 2400: 0.3425833448679819\n",
      "Loss at epoch 2500: 0.3367523435859255\n",
      "Loss at epoch 2600: 0.37487145410824974\n",
      "Loss at epoch 2700: 0.3660632401692701\n",
      "Loss at epoch 2800: 0.3618879693888374\n",
      "Loss at epoch 2900: 0.38081549182287666\n",
      "Loss at epoch 3000: 0.3480694338137813\n",
      "Loss at epoch 3100: 0.35908447399502463\n",
      "Loss at epoch 3200: 0.36107165174421635\n",
      "Loss at epoch 3300: 0.33873316801435055\n",
      "Loss at epoch 3400: 0.3429970192707988\n",
      "Loss at epoch 3500: 0.338841241584467\n",
      "Loss at epoch 3600: 0.32668313102401747\n",
      "Loss at epoch 3700: 0.3666416846390774\n",
      "Loss at epoch 3800: 0.36688872451156057\n",
      "Loss at epoch 3900: 0.3768474645394847\n",
      "Loss at epoch 4000: 0.3548011465410556\n",
      "Loss at epoch 4100: 0.33448878339670995\n",
      "Loss at epoch 4200: 0.35165997197636956\n",
      "Loss at epoch 4300: 0.34950591156428185\n",
      "Loss at epoch 4400: 0.34561886120808244\n",
      "Loss at epoch 4500: 0.35959938246271955\n",
      "Loss at epoch 4600: 0.3625845056864994\n",
      "Loss at epoch 4700: 0.39075275336393933\n",
      "Loss at epoch 4800: 0.35247585665451664\n",
      "Loss at epoch 4900: 0.33930633537670624\n",
      "Fold 3 - Accuracy: 0.8814, Precision: 0.0000, Recall: 0.0000\n",
      "Training on fold 4: 28935 samples\n",
      "Loss at epoch 0: 0.6914510033059919\n",
      "Loss at epoch 100: 0.5789930774272836\n",
      "Loss at epoch 200: 0.5134404627573627\n",
      "Loss at epoch 300: 0.45378775786101505\n",
      "Loss at epoch 400: 0.42580930323438027\n",
      "Loss at epoch 500: 0.41363507009422235\n",
      "Loss at epoch 600: 0.4029067491330842\n",
      "Loss at epoch 700: 0.3860799854640233\n",
      "Loss at epoch 800: 0.38721160785438324\n",
      "Loss at epoch 900: 0.3658039001267692\n",
      "Loss at epoch 1000: 0.3895452292485362\n",
      "Loss at epoch 1100: 0.37268829037141105\n",
      "Loss at epoch 1200: 0.3639567697341595\n",
      "Loss at epoch 1300: 0.3832263420342437\n",
      "Loss at epoch 1400: 0.3683700838692281\n",
      "Loss at epoch 1500: 0.3666205837297255\n",
      "Loss at epoch 1600: 0.35695048808982804\n",
      "Loss at epoch 1700: 0.36930571183050004\n",
      "Loss at epoch 1800: 0.36328921719381035\n",
      "Loss at epoch 1900: 0.3978343195772028\n",
      "Loss at epoch 2000: 0.36467938489948754\n",
      "Loss at epoch 2100: 0.3512122777891464\n",
      "Loss at epoch 2200: 0.3894244331955914\n",
      "Loss at epoch 2300: 0.3855756285473564\n",
      "Loss at epoch 2400: 0.38260584905792205\n",
      "Loss at epoch 2500: 0.3699812941318306\n",
      "Loss at epoch 2600: 0.3818438660448233\n",
      "Loss at epoch 2700: 0.33958534883375885\n",
      "Loss at epoch 2800: 0.37203008262614673\n",
      "Loss at epoch 2900: 0.3512774003746163\n",
      "Loss at epoch 3000: 0.3511164695653549\n",
      "Loss at epoch 3100: 0.36291289953430483\n",
      "Loss at epoch 3200: 0.36801738629295433\n",
      "Loss at epoch 3300: 0.38084847727765503\n",
      "Loss at epoch 3400: 0.36785434419971436\n",
      "Loss at epoch 3500: 0.358805707033363\n",
      "Loss at epoch 3600: 0.3380358180708266\n",
      "Loss at epoch 3700: 0.33405908399236595\n",
      "Loss at epoch 3800: 0.3548265323995087\n",
      "Loss at epoch 3900: 0.36294085316844454\n",
      "Loss at epoch 4000: 0.34504015165409896\n",
      "Loss at epoch 4100: 0.3647675152972356\n",
      "Loss at epoch 4200: 0.37572634632195556\n",
      "Loss at epoch 4300: 0.3557920829972964\n",
      "Loss at epoch 4400: 0.37074242984889894\n",
      "Loss at epoch 4500: 0.3587880395255179\n",
      "Loss at epoch 4600: 0.35583364105485144\n",
      "Loss at epoch 4700: 0.3326700944097212\n",
      "Loss at epoch 4800: 0.3205957637419054\n",
      "Loss at epoch 4900: 0.3627224738014172\n",
      "Fold 4 - Accuracy: 0.8868, Precision: 0.0000, Recall: 0.0000\n",
      "Training on fold 5: 28935 samples\n",
      "Loss at epoch 0: 0.6925621264873573\n",
      "Loss at epoch 100: 0.5770068635833904\n",
      "Loss at epoch 200: 0.5051329484655668\n",
      "Loss at epoch 300: 0.46208341727756497\n",
      "Loss at epoch 400: 0.4304674304456583\n",
      "Loss at epoch 500: 0.4071037327427972\n",
      "Loss at epoch 600: 0.39287870588458174\n",
      "Loss at epoch 700: 0.37132832979425895\n",
      "Loss at epoch 800: 0.35984051165655184\n",
      "Loss at epoch 900: 0.38630472717240444\n",
      "Loss at epoch 1000: 0.37606939017629687\n",
      "Loss at epoch 1100: 0.3546128901919408\n",
      "Loss at epoch 1200: 0.3716393407831123\n",
      "Loss at epoch 1300: 0.3779415073199251\n",
      "Loss at epoch 1400: 0.3612300042269718\n",
      "Loss at epoch 1500: 0.35940352813362625\n",
      "Loss at epoch 1600: 0.38959584843799566\n",
      "Loss at epoch 1700: 0.3544465001656387\n",
      "Loss at epoch 1800: 0.3614986471727084\n",
      "Loss at epoch 1900: 0.3546173687965293\n",
      "Loss at epoch 2000: 0.3598465127337594\n",
      "Loss at epoch 2100: 0.36828516136362194\n",
      "Loss at epoch 2200: 0.3816028431118595\n",
      "Loss at epoch 2300: 0.35750426665358465\n",
      "Loss at epoch 2400: 0.3563168035240668\n",
      "Loss at epoch 2500: 0.3630980258409628\n",
      "Loss at epoch 2600: 0.3620679523333715\n",
      "Loss at epoch 2700: 0.3629692866554417\n",
      "Loss at epoch 2800: 0.352163394329599\n",
      "Loss at epoch 2900: 0.3578560544273565\n",
      "Loss at epoch 3000: 0.3648398370119294\n",
      "Loss at epoch 3100: 0.35904348639755085\n",
      "Loss at epoch 3200: 0.36474884724100015\n",
      "Loss at epoch 3300: 0.3865855849961695\n",
      "Loss at epoch 3400: 0.34881132148872457\n",
      "Loss at epoch 3500: 0.347725060074491\n",
      "Loss at epoch 3600: 0.36055508420791954\n",
      "Loss at epoch 3700: 0.34464729682843814\n",
      "Loss at epoch 3800: 0.3669045626608916\n",
      "Loss at epoch 3900: 0.37165330729513063\n",
      "Loss at epoch 4000: 0.3785840317848338\n",
      "Loss at epoch 4100: 0.3476211536161716\n",
      "Loss at epoch 4200: 0.35453343613201177\n",
      "Loss at epoch 4300: 0.36968720249806747\n",
      "Loss at epoch 4400: 0.35552812813925483\n",
      "Loss at epoch 4500: 0.36361079086065884\n",
      "Loss at epoch 4600: 0.3655939329243292\n",
      "Loss at epoch 4700: 0.34547586805068237\n",
      "Loss at epoch 4800: 0.3605886535158544\n",
      "Loss at epoch 4900: 0.3574026233390565\n",
      "Fold 5 - Accuracy: 0.8828, Precision: 0.0000, Recall: 0.0000\n",
      "\n",
      "Average Accuracy: 0.8831564702391402\n",
      "Average Precision: 0.0\n",
      "Average Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Assume X and y are your original DataFrames/Series.\n",
    "# Factorize each categorical column in X to ensure numeric values.\n",
    "X_factorized = X.copy()\n",
    "for col in X.columns:\n",
    "    X_factorized[col], _ = pd.factorize(X[col])\n",
    "\n",
    "# Factorize and one-hot encode the target variable.\n",
    "y_numeric, uniques = pd.factorize(y.values.ravel())\n",
    "y_tensor = torch.tensor(y_numeric, dtype=torch.long)\n",
    "y_onehot = F.one_hot(y_tensor, num_classes=2).numpy()\n",
    "\n",
    "# Create train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_factorized, y_onehot, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# --- Neural Network Class Definition ---\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, lr=0.001):\n",
    "        self.lr = lr\n",
    "        # Initialize weights with small random values and biases with zeros.\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1) * lr\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2) * lr\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "        \n",
    "        self.W3 = np.random.randn(hidden_size2, output_size) * lr\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Implement a 3 layer neural network forward propagation\"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        self.X = X\n",
    "        self.z1 = np.dot(self.X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        output = self.softmax(self.z3)\n",
    "        return output \n",
    "    \n",
    "    def compute_loss(self, y, output, weight=None):\n",
    "        \"\"\"\n",
    "        Compute Cross Entropy Loss, where y is one-hot encoded and output is the softmax output.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        if weight is not None:\n",
    "            weight = weight.reshape(1, -1)\n",
    "            loss = -1/m * np.sum(weight * (y * np.log(output)))\n",
    "        else:\n",
    "            loss = -1/m * np.sum(y * np.log(output))\n",
    "        return loss\n",
    "            \n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = y.shape[0]\n",
    "        # Output layer gradients\n",
    "        dZ3 = output - y\n",
    "        dW3 = np.dot(self.a2.T, dZ3) / m\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Propagate to second hidden layer\n",
    "        dA2 = np.dot(dZ3, self.W3.T)\n",
    "        dZ2 = dA2 * (self.a2 * (1 - self.a2))\n",
    "        dW2 = np.dot(self.a1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Propagate to first hidden layer\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * (self.a1 * (1 - self.a1))\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.W3 -= self.lr * dW3\n",
    "        self.b3 -= self.lr * db3\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size=64):\n",
    "        \"\"\"Batch Gradient Descent training\"\"\"\n",
    "        m = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.choice(m, batch_size, replace=False)\n",
    "            # Use .iloc for DataFrame and direct indexing for numpy arrays.\n",
    "            X_batch = X.iloc[indices]\n",
    "            y_batch = y[indices]\n",
    "        \n",
    "            output = self.forward(X_batch)\n",
    "            loss = self.compute_loss(y_batch, output)\n",
    "            self.backward(X_batch, y_batch, output)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Loss at epoch {epoch}: {loss}')\n",
    "                \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return output\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        pred = np.argmax(self.predict(X), axis=1)\n",
    "        true_labels = np.argmax(y, axis=1)\n",
    "        return np.mean(pred == true_labels)\n",
    "    \n",
    "    def precision(self, X, y):\n",
    "        preds = np.argmax(self.predict(X), axis=1)\n",
    "        true_labels = np.argmax(y, axis=1)\n",
    "        tp = np.sum((preds == 1) & (true_labels == 1))\n",
    "        fp = np.sum((preds == 1) & (true_labels == 0))\n",
    "        return tp / (tp + fp + 1e-8)\n",
    "        \n",
    "    def recall(self, X, y):\n",
    "        preds = np.argmax(self.predict(X), axis=1)\n",
    "        true_labels = np.argmax(y, axis=1)\n",
    "        tp = np.sum((preds == 1) & (true_labels == 1))\n",
    "        fn = np.sum((preds == 0) & (true_labels == 1))\n",
    "        return tp / (tp + fn + 1e-8)\n",
    "\n",
    "# --- Cross-Validation Training ---\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "input_size = 16\n",
    "output_size = 2\n",
    "hidden_size1 = 32\n",
    "hidden_size2 = 16\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "fold = 1\n",
    "\n",
    "# Lists to collect metrics for each fold\n",
    "acc_scores = []\n",
    "prec_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "# Reset indices for X_train to ensure proper integer indexing\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "# y_train is already a NumPy array\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Use .iloc for DataFrame indexing and numpy indexing for y_train\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    nn = NN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    print(f\"Training on fold {fold}: {X_tr.shape[0]} samples\")\n",
    "    \n",
    "    nn.train(X_tr, y_tr, epochs=5000, batch_size=2000)\n",
    "    \n",
    "    # Evaluate on the validation set from the current fold\n",
    "    acc = nn.accuracy(X_val, y_val)\n",
    "    prec = nn.precision(X_val, y_val)\n",
    "    rec = nn.recall(X_val, y_val)\n",
    "    \n",
    "    print(f\"Fold {fold} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "    \n",
    "    acc_scores.append(acc)\n",
    "    prec_scores.append(prec)\n",
    "    recall_scores.append(rec)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", np.mean(acc_scores))\n",
    "print(\"Average Precision:\", np.mean(prec_scores))\n",
    "print(\"Average Recall:\", np.mean(recall_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
